{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Artififial Intelligence - COMPSCI4004 & COMPSCI5087 2019-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Lab Week 2: Introduction - PEAS and the Open AI Gym environment \n",
    "<font color=red>Solutions</font><br>\n",
    "<font size=1>v2019-2020b</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives:\n",
    "- Familiarise yourself with Open AI Gym and get the Python package installed on your account/computer.\n",
    "- Obtain an understanding of the Open AI Gym by inspecting the documentation and running a basic tutorial.\n",
    "- Analyse a specific problem using the PEAS framework to identify the performance measure, environment (including a task description), sensors and actuators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guide**: The notebook contains specific tasks you'll need to carry out to make the notebook run. These are indicated with:\n",
    "\n",
    "* <font color=dark-magenta>TASK:</font> This is a task for you to carry out before proceeding. \n",
    "* <font color=green>CHECKPOINTS:</font> This indicates a key point you should understand before proceeding. If you're in soubt then ask then consult the lab assistants.\n",
    "* A basic model solution (marked with <font color=red>SOLUTION</font>) will be provided a week after the Lab session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q2.1 Prerequisites\n",
    "- <font color=dark-magenta>TASK:</font> Before installing the Open AI Gym environment make sure you have a Python 3.6+ environment installed on your computer.\n",
    "\n",
    " - Lab machines: Python 3.7 is installed (find Anaconda promt via the start menu)\n",
    " - Linux or Windows: We recommend installing [Anaconda](https://www.anaconda.com/download/) (Python 3.6+) which includes numpy and other useful libraries that you would otherwise have to install yourself.\n",
    " - MacOS: We generally recommend using the already installed Python and simply updating the packages as needed.\n",
    "\n",
    "\n",
    "*Warning*: We only support the the Lab machines (Windows) and it is your own responsibility if you want to install the environemnt on your own setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q2.2: Installing Open AI Gym...\n",
    "- <font color=dark-magenta>TASK:</font> Install the Pyhton-based Open AI Gym. An installation guide can be found at https://gym.openai.com/docs/ (avoid building from source if possible). \n",
    "    - On the lab machines: Run \"pip install --user gym[atari]\" from an Anaconda prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q2.3: Complete the Open Gym Tutorial - the Cartpole environment\n",
    "- <font color=dark-magenta>TASK:</font> Step through the CartPole tutorial and run the code included in the cell below (from the tutorial). The tutorial is based on the CartPole example and we recommend reading the problem [description](https://gym.openai.com/envs/CartPole-v1/) and inspecting the environment definition (Python code; available via [github](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py))\n",
    " - Note: Depending on your programming background you may find it useful using standard Python (i.e., command line or an IDE, e.g. [PyCharm](https://www.jetbrains.com/pycharm/) or [vscode](https://code.visualstudio.com/)) when programming against the Open AI gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\Anaconda3_36_520\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "for step in range(200):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # take random action\n",
    "    observation, reward, done, info = env.step(action) \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=green>CHECKPOINT:</font> You should see a new window open up and a cartpole starting to move around. You may see a warning because the CartPole keeps moving even though it has reached its goal. If that is the case, then you're safe to move on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution below includes a print out of some relevant information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=0:\n",
      "\tAction=0\n",
      "\tObservation=[ 0.01670041 -0.23361276  0.02089756  0.31793424] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=1:\n",
      "\tAction=1\n",
      "\tObservation=[ 0.01202815 -0.03879458  0.02725624  0.03191423] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=2:\n",
      "\tAction=0\n",
      "\tObservation=[ 0.01125226 -0.23429658  0.02789453  0.33307065] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=3:\n",
      "\tAction=0\n",
      "\tObservation=[ 0.00656633 -0.42980423  0.03455594  0.63441796] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=4:\n",
      "\tAction=1\n",
      "\tObservation=[-0.00202976 -0.2351809   0.0472443   0.35281467] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=5:\n",
      "\tAction=1\n",
      "\tObservation=[-0.00673337 -0.04076149  0.05430059  0.07539561] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=6:\n",
      "\tAction=1\n",
      "\tObservation=[-0.0075486   0.15354167  0.05580851 -0.19967301] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=7:\n",
      "\tAction=0\n",
      "\tObservation=[-0.00447777 -0.04233224  0.05181505  0.1100796 ] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=8:\n",
      "\tAction=1\n",
      "\tObservation=[-0.00532441  0.15201041  0.05401664 -0.16581632] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=9:\n",
      "\tAction=0\n",
      "\tObservation=[-0.00228421 -0.04384149  0.05070031  0.14340622] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=10:\n",
      "\tAction=0\n",
      "\tObservation=[-0.00316104 -0.2396515   0.05356844  0.45164331] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=11:\n",
      "\tAction=1\n",
      "\tObservation=[-0.00795407 -0.04532644  0.0626013   0.17631543] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=12:\n",
      "\tAction=0\n",
      "\tObservation=[-0.00886059 -0.24128582  0.06612761  0.4880715 ] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=13:\n",
      "\tAction=1\n",
      "\tObservation=[-0.01368631 -0.04715614  0.07588904  0.21694078] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=14:\n",
      "\tAction=1\n",
      "\tObservation=[-0.01462943  0.14680352  0.08022786 -0.05087137] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=15:\n",
      "\tAction=1\n",
      "\tObservation=[-0.01169336  0.34068891  0.07921043 -0.3172023 ] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=16:\n",
      "\tAction=1\n",
      "\tObservation=[-0.00487959  0.53459854  0.07286638 -0.58389122] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=17:\n",
      "\tAction=0\n",
      "\tObservation=[ 0.00581239  0.33853552  0.06118856 -0.26917333] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=18:\n",
      "\tAction=1\n",
      "\tObservation=[ 0.0125831   0.53273332  0.05580509 -0.54194667] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=19:\n",
      "\tAction=1\n",
      "\tObservation=[ 0.02323776  0.72702836  0.04496616 -0.81653736] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=20:\n",
      "\tAction=0\n",
      "\tObservation=[ 0.03777833  0.53132057  0.02863541 -0.51005681] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=21:\n",
      "\tAction=0\n",
      "\tObservation=[ 0.04840474  0.33580716  0.01843428 -0.20848927] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=22:\n",
      "\tAction=0\n",
      "\tObservation=[0.05512088 0.14042654 0.01426449 0.08995124] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=23:\n",
      "\tAction=1\n",
      "\tObservation=[ 0.05792941  0.33534115  0.01606352 -0.19819732] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=24:\n",
      "\tAction=0\n",
      "\tObservation=[0.06463624 0.13999317 0.01209957 0.09950932] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=25:\n",
      "\tAction=0\n",
      "\tObservation=[ 0.0674361  -0.05530008  0.01408976  0.39598494] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=26:\n",
      "\tAction=0\n",
      "\tObservation=[ 0.0663301  -0.25061908  0.02200945  0.69307661] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=27:\n",
      "\tAction=0\n",
      "\tObservation=[ 0.06131772 -0.44603934  0.03587099  0.99260626] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=28:\n",
      "\tAction=0\n",
      "\tObservation=[ 0.05239693 -0.64162241  0.05572311  1.29633595] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=29:\n",
      "\tAction=1\n",
      "\tObservation=[ 0.03956448 -0.44725076  0.08164983  1.02160513] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=30:\n",
      "\tAction=0\n",
      "\tObservation=[ 0.03061947 -0.64336     0.10208193  1.33876737] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=31:\n",
      "\tAction=0\n",
      "\tObservation=[ 0.01775227 -0.8396085   0.12885728  1.66156765] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=32:\n",
      "\tAction=0\n",
      "\tObservation=[ 9.60097567e-04 -1.03597469e+00  1.62088634e-01  1.99145423e+00] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=33:\n",
      "\tAction=0\n",
      "\tObservation=[-0.0197594  -1.23238205  0.20191772  2.32964928] \n",
      "\tReward=1.0\n",
      "\tDone=False\n",
      "\n",
      "Step=34:\n",
      "\tAction=1\n",
      "\tObservation=[-0.04440704 -1.03958333  0.2485107   2.10528786] \n",
      "\tReward=1.0\n",
      "\tDone=True\n",
      "\n",
      "Done; exiting\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "for step in range(200):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # take a random action from the available actions\n",
    "    observation, reward, done, info = env.step(action) \n",
    "    print(\"Step=%d:\\n\\tAction=%s\\n\\tObservation=%s \\n\\tReward=%s\\n\\tDone=%s\\n\" % (step,action,observation,reward,done)) # this part is optional\n",
    "    if done:\n",
    "        print(\"Done; exiting\")\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "- The main point of this task is simply to see that the Open AI Gym has been installed correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q2.4: PEAS Analysis of The Frozen Lake Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will analyse the Frozen Lake enviroment from the Open AI Gym. The goal is to make a PEAS analysis of this environment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2.4.1: Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=dark-magenta>TASK: </font> Explore the [Frozen Lake environment](https://gym.openai.com/envs/FrozenLake-v0/) in the Open AI Gym and make sure to inspect the [source code](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py). Make an initial  analysis of the Task Environment by identifying the states, actions and rewards in this environment. Executing the code below may help you appreciate how the environment is represented in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Observation/states: Discrete(16)\n",
      "Action space: Discrete(4)\n",
      "Rewards: (0, 1)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "print(\"--------------------------------\") \n",
    "env.render()\n",
    "print(\"\\n--------------------------------\\n\")\n",
    "\n",
    "print(\"Observation/states: %s\" % (env.observation_space))\n",
    "print(\"Action space: %s\" % (env.action_space))\n",
    "print(\"Rewards: \" + str(env.reward_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>\n",
    "\n",
    "States: S, F, H G <br>\n",
    "Actions: \"Left\",\"Down\",\"Right\",\"Up\" (0,1,2,3) <br>\n",
    "Rewards are recieved from the environment and effectively defines the performance measure for us: S=0 (starting state), F= 0, H=0 (terminal state), G= 1 (terminal state)<br>\n",
    "\n",
    "<br>\n",
    "<div style=\"border:2px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2.4.2: Implement an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=dark-magenta>TASK: </font>  Implement an agent that takes random actions for the Frozen lake environment (simliar to the CartPole example). \n",
    "Your agent should display:\n",
    " * a rendering of the environment (use 'env.render()' for this)\n",
    " * the observation/state (a coordinate or similar)\n",
    " * the action\n",
    " * the reward obtained\n",
    " * an indication if the agent has succeeded (env.step() usually returns this information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INSERT YOUR CODE HERE]\n"
     ]
    }
   ],
   "source": [
    "print(\"[INSERT YOUR CODE HERE]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "\n",
      "Starting Condition:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--------------------------------\n",
      "\n",
      "Computing...: The agent has decided on the following action:\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "..so the new state of the world is \n",
      "\tState=1 \n",
      "\tReward=0.0\n",
      "\tDone=False\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Computing...: The agent has decided on the following action:\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "..so the new state of the world is \n",
      "\tState=1 \n",
      "\tReward=0.0\n",
      "\tDone=False\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Computing...: The agent has decided on the following action:\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "..so the new state of the world is \n",
      "\tState=0 \n",
      "\tReward=0.0\n",
      "\tDone=False\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Computing...: The agent has decided on the following action:\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "..so the new state of the world is \n",
      "\tState=1 \n",
      "\tReward=0.0\n",
      "\tDone=False\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "Computing...: The agent has decided on the following action:\n",
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "..so the new state of the world is \n",
      "\tState=5 \n",
      "\tReward=0.0\n",
      "\tDone=True\n",
      "\n",
      "--------------------------------\n",
      "Sorry, you did not make it - try again\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "\n",
    "print(\"--------------------------------\") # this part is optional\n",
    "print(\"\\nStarting Condition:\")\n",
    "env.render()\n",
    "done = False\n",
    "while not done: \n",
    "    action = env.action_space.sample() # take random action from the available actions\n",
    "    \n",
    "    print(\"--------------------------------\") \n",
    "    print(\"\\nComputing...: The agent has decided on the following action:\")\n",
    "   \n",
    "    observation, reward, done, info = env.step(action)     \n",
    "    \n",
    "    env.render()\n",
    "        \n",
    "    print(\"\\n..so the new state of the world is \\n\\tState=%s \\n\\tReward=%s\\n\\tDone=%s\\n\" % (observation,reward, done)) # this part is optional\n",
    "    \n",
    "    if done:\n",
    "        print(\"--------------------------------\") \n",
    "        if reward == 1.0:\n",
    "            print(\"Great you made it across the lake\")\n",
    "        else:\n",
    "            print(\"Sorry, you did not make it - try again\")\n",
    "        print(\"--------------------------------\") \n",
    "           \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=green>CHECKPOINT: </font> Rerun you agent a couple of times and observe the percept and action sequences to make sure it performs as expected.... Hint: is the environment deterministic or stochastic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "- You should see that the agent does not end up in the state you expected when telling it to carry out a specific action because the ice is slippery.\n",
    "\n",
    "- By rerunning the agent multiple tiles you should observe that the agent also most never reseraches (it can sometimes take hundres or even thousands of reruns to find a random sequanc of (minimum) 6 actions that would get you to the goal.\n",
    "\n",
    "- The code below reruns the agents until the goal is reached once - observe the final number of restarts and actions needed to get to the goal: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Restart #1: Sorry, you did not make it (with 8 actions) - try again\n",
      "--------------------------------\n",
      "Restart #2: Sorry, you did not make it (with 2 actions) - try again\n",
      "--------------------------------\n",
      "Restart #3: Sorry, you did not make it (with 4 actions) - try again\n",
      "--------------------------------\n",
      "Restart #4: Sorry, you did not make it (with 5 actions) - try again\n",
      "--------------------------------\n",
      "Restart #5: Sorry, you did not make it (with 7 actions) - try again\n",
      "--------------------------------\n",
      "Restart #6: Sorry, you did not make it (with 11 actions) - try again\n",
      "--------------------------------\n",
      "Restart #7: Sorry, you did not make it (with 5 actions) - try again\n",
      "--------------------------------\n",
      "Restart #8: Sorry, you did not make it (with 11 actions) - try again\n",
      "--------------------------------\n",
      "Restart #9: Sorry, you did not make it (with 6 actions) - try again\n",
      "--------------------------------\n",
      "Restart #10: Sorry, you did not make it (with 10 actions) - try again\n",
      "--------------------------------\n",
      "Restart #11: Sorry, you did not make it (with 6 actions) - try again\n",
      "--------------------------------\n",
      "Restart #12: Sorry, you did not make it (with 4 actions) - try again\n",
      "--------------------------------\n",
      "Restart #13: Sorry, you did not make it (with 6 actions) - try again\n",
      "--------------------------------\n",
      "Restart #14: Sorry, you did not make it (with 3 actions) - try again\n",
      "--------------------------------\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Restart #15: Great, you made it across the lake with 8 actions\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "\n",
    "n_restarts = 0\n",
    "goal_found_once = False\n",
    "\n",
    "while not goal_found_once: \n",
    "    n_restarts += 1\n",
    "    done = False\n",
    "    n_actions_for_episode = 0\n",
    "    while not done: \n",
    "        n_actions_for_episode += 1\n",
    "        action = env.action_space.sample() # take random action from the available actions\n",
    "        observation, reward, done, info = env.step(action)     \n",
    "\n",
    "        if done:\n",
    "            print(\"--------------------------------\") \n",
    "            if reward == 1.0:                \n",
    "                env.render()\n",
    "                print(\"\\nRestart #%s: Great, you made it across the lake with %d actions\" % (n_restarts,n_actions_for_episode))                \n",
    "                goal_found_once = True                \n",
    "            else:\n",
    "                print(\"Restart #%s: Sorry, you did not make it (with %d actions) - try again\" % (n_restarts,n_actions_for_episode))\n",
    "                env.reset()\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "- Is this a good agent? Probably not! In furture labs we will see how we (in certain conditions) can come up with a better agent than the random one implemented here).\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"border:2px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2.4.3: Full PEAS Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=dark-magenta>TASK: </font> Perform a full PEAS analysis (see the AIMA book chapter 2, or the lecture slides from week 2) to identify the\n",
    "    \n",
    "    - Performance measure \n",
    "    - Environment (including a full description of the environment)\n",
    "    - Actuators (which actions can the agent take) ?\n",
    "    - Sensors, i.e., what sensory input/observations are available to the agent (in the current Open AI Gym environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>\n",
    "\n",
    "\n",
    "- Performance measure: <br>\n",
    " - Obtain the maximum reward (get across the lake) without falling into the lake (death... or at least restart). IN this case the reward is directly observed  from the environment which is handy but not always the case. One would perhaps expect that falling into the lake would give a negative reward; however, that is not the way this specific env works.\n",
    "\n",
    "- Sensors, i.e., what sensory input/observations are available to the agent (in the current Open AI Gym environment)<br>\n",
    " - We can only see where we are right now not any holes around us (the rationale would be that the ice hides any holes).\n",
    " - Note: You could argue that in the real world we would be able to observe that some areas around us would be dangerous; however, that is not the way *this* environment works, and it would be catastrophic to design an agent-based on wishful thinking.\n",
    "\n",
    "- Actuators<br>\n",
    " - Virtual commands (up, down, left, right) leading the virtual agent to change position.\n",
    "\n",
    "- Environment (including a full description of the environment)<br>\n",
    " - Basic description: A virtual simulation environment of a lake in a 4x4 layout with four possible states; holes (H), free (F) and a single final goal (reward +1). It is possible to take four virtual actions (up,down,lef, right); there is no cost associated with an action. \n",
    "<br>\n",
    " - Stochastic vs deterministic: It is highly stochastic; even if you tell the agent to go down the ice is so slippery that you sometimes end up on the left or rigth. The probability of ending up in the desired state is only 1/3 (i.e. 33.33%).\n",
    " - Fully observable in the sense that you know exactly where you are (locally), however, partially observable in the sense that you do not know the full map or what's around you (globally).\n",
    " - Static vs dynamic: Well, that depends on the viewpoint. If you have infinite steps until the agent terminates, then we consider it static as it does not change while we are deciding which action(s) to take. However, if you placed a limit on the number of steps (in a for-loop, say) and the agent is aware of this then it is strictly speaking a semi-static environment since time matters and it would objectively change the optimal stategy. However, you have most likely chosen (as a designing) to ignore all of this since you have gone for a random agent which does not consider these aspects. Regardless, something to be mentioned and considered in a complete and rigorous analysis.\n",
    " - Discrete/continuous: Discrete (in terms of most aspects; states, available sensor inputs and actions). The only aspect which could potentially be continuous is time but this has been quantized into discrete steps in the environment and is not a concern for the agent (or its designer).\n",
    " - Single-agent vs multiple agents: Single (we are not competing against anyone)\n",
    " - Known: We can assume to know the rules of this game. In a realy situation, we often need to go learn these.\n",
    " \n",
    " \n",
    "<b>Notice:</b> In analysing the task environment it is important to justify how you came to a conclusion and from which viewpoint you made the analysis (e.g. an existing agent with predefined sensors and functionality - or as an external observer).\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"border:2px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
